;モンテカルロ法
+var state_value: []float
var log: list<[]int>
var mont: [][]float
+class Learn()
	var count: int
	var step: int
	
	*func ctor()
		do @log :: #list<[]int>
		do @state_value :: #[\game@Xm * \game@Ym]float
		do @mont :: #[\game@Xm * \game@Ym, 4]float
	end func
	+func init(p: \game@Player, t: \game@Target)
		do me.update(p, t)
		do me.count :+ 1
		do dbg@print("count:\{me.count},step:\{me.step}で終了しました。\n")
		do me.step :: 0
	end func
	
	
	+func learn(p: \game@Player, t: \game@Target): int
		do me.step :+ 1
		
		var a: int
		var Qmax: float :: -inf
		var Qmax_list: []float :: #[4]float
		do Qmax_list.fill(-inf)
		
		;今の場所から次の場所へ移動したとき
		for i(0, 3)
			var pos: []int :: me.get_next_pos(p.x, p.y, i)
			if(me.out(pos[0], pos[1]) <> -1)
				;次の場所の行動の最大のQ値を求める
				for i2(0, 3)
					;次の次の状態が行動可能か確認
					var pos2: []int :: me.get_next_pos(pos[0], pos[1], i2)
					if(me.out(pos2[0], pos2[1]) <> -1)
						;次の状態の行動のQ値入手
						var Q: float :: @mont[pos[0] + pos[1] * \game@Xm][i2]
						if(Q > Qmax_list[i])
							do Qmax_list[i] :: Q
						end if
					end if
				end for
				if(Qmax_list[i] > Qmax)
					do Qmax :: Qmax_list[i]
					do a :: i
				end if
			end if
		end for
		;今の状態からとれる行動の最大のQ値を状態値とする
		var max: float
		for i(0, 3)
			var Q: float :: @mont[p.x + p.y * \game@Xm][i]
			if(Q > max | i = 0)
				do max :: Q
				do @state_value[p.x + p.y * \game@Xm] :: max
			end if
		end for
		
		;行動できるところへ20%の確率でランダム行動
		if(lib@rnd(1, 10) <= 2)
			var pos: []int
			while(me.out(pos[0], pos[1]) = -1, skip)
				do a :: lib@rnd(0, 3)
				do pos :: me.get_next_pos(p.x, p.y, a)
			end while
		end if
		;状態行動記録
		do @log.add([p.x, p.y, a])
		
		ret a
	end func
	
	+func update(p: \game@Player, t: \game@Target)
		var Qsum: float
		var Q: []float :: #[1]float
		var trace: [][]int :: #[\game@Xm * \game@Ym, 4]int
		
		do @log.tail()
		while(!@log.term())
			var log: []int :: @log.get()
			;ゴールか辿ってまだ評価していない状態行動を評価する
			if(trace[log[0] + log[1] * \game@Xm][log[2]] <> 1)
				;報酬
				var r: float :: me.get_reward(log, t)
				;累積Q値の時間割引
				do Qsum :* 0.8
				;Q値更新（保存された値からモンテカルロ法で更新）
				do @mont[log[0] + log[1] * \game@Xm][log[2]] :+ 0.2 * (r + Qsum - @mont[log[0] + log[1] * \game@Xm][log[2]])
				;累積Q値の更新
				do Qsum :+ r
				
				;do dbg@print("[\{log[0]}][\{log[1]}][\{log[2]}]:\{@mont[log[0] + log[1] * \game@Xm][log[2]]}\n")
				
			end if
			;辿ったしるしを残す
			do trace[log[0] + log[1] * \game@Xm][log[2]] :: 1
			
			do @log.prev()
		end while
		;一回のゲームで記録した状態行動を削除
		do @log.head()
		while(!@log.term())
			do @log.del()
		end while
		
		
	end func
	
	+func out(x: int, y: int): int
		if(x > \game@Xm - 1 | x < 0 | y > \game@Ym - 1 | y < 0)
			ret - 1
		end if
		ret 0
	end func
	
	+func draw(x: int, y: int, width: float, height: float, font: draw@Font)
		do font.draw(x $ float * width + width / 2.0 + width / 8.0 * 3.0, y $ float * height + height / 2.0, "\{@mont[x + y * \game@Ym][0]}", 0xFF000000)
		do font.draw(x $ float * width + width / 2.0 - width / 8.0 * 3.0, y $ float * height + height / 2.0, "\{@mont[x + y * \game@Ym][1]}", 0xFF000000)
		do font.draw(x $ float * width + width / 2.0, y $ float * height + height / 2.0 - height / 8.0 * 3.0, "\{@mont[x + y * \game@Ym][2]}", 0xFF000000)
		do font.draw(x $ float * width + width / 2.0, y $ float * height + height / 2.0 + height / 8.0 * 3.0, "\{@mont[x + y * \game@Ym][3]}", 0xFF000000)
		
		do font.draw(x $ float * width + width / 2.0, y $ float * height + height / 2.0, "\{@state_value[x + y * \game@Ym]}", 0xFF000000)
		
	end func
	
	+func get_reward(log: []int, t: \game@Target): float
		if(log[0] = t.x & log[1] = t.y)
			ret 10.0
		end if
		ret 0.0
	end func
	+func get_next_pos(x: int, y: int, a: int): []int
		switch(a)
		case 0
			do x :+ 1
		case 1
			do x :- 1
		case 2
			do y :- 1
		case 3
			do y :+ 1
		end switch
		ret[x, y]
	end func
	
end class
